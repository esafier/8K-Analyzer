8K ANALYZER — LLM INTEGRATION CHANGELOG
========================================
Date: January 29, 2026

WHAT THIS PROGRAM DOES
----------------------
This tool monitors SEC 8-K filings for executive changes and compensation
events. It pulls filings from the SEC EDGAR database, filters them for
relevance, summarizes them, stores them in a local SQLite database, and
displays them in a web dashboard.


WHAT CHANGED (SUMMARY)
----------------------
We added an LLM (GPT-4o mini via OpenAI API) as a third filtering/summarization
stage. The old keyword-based filtering still runs first, then the LLM reviews
the results for better accuracy and writes natural language summaries.


THE OLD PIPELINE (before changes)
---------------------------------
  SEC EDGAR --> Stage 1: Item Code Filter --> Stage 2: Keyword Filter --> Sentence Scorer Summary --> Database

  Stage 1: Checks filing metadata for target item codes (5.02, 1.01, 1.02, 8.01).
           ~100 filings/day --> ~10 pass. Free, instant.

  Stage 2: Downloads filing text and scans for 50+ hardcoded keywords organized
           by category (Management Change, Compensation). ~10 --> ~5 pass.
           5.02 filings auto-pass even without keyword matches.

  Summary: A scoring algorithm in summarizer.py picks the "best" 2 sentences
           from the filing. Scores sentences by: keyword matches (+2), early
           position (+1), executive role mentions (+1), person names (+2),
           boilerplate phrases (-1). Returns the top 2 sentences trimmed to
           150 characters.

  Problems with the old system:
  - Keyword matching misses filings with unusual phrasing
  - Sentence scorer produces choppy, sometimes irrelevant summaries
  - Categories/subcategories sometimes wrong when keywords are ambiguous


THE NEW PIPELINE (after changes)
---------------------------------
  SEC EDGAR --> Stage 1: Item Code Filter --> Stage 2: Keyword Filter --> Stage 3: LLM Review --> Database

  Stage 1: UNCHANGED.

  Stage 2: MOSTLY UNCHANGED. Still does keyword filtering. Two differences:
           - Filings that pass keywords are sent to Stage 3 (instead of going
             straight to the database)
           - "Near-miss" filings are now tracked: filings with item codes 5.02
             or 8.01 that FAIL keyword matching are also sent to Stage 3.
             Previously these 5.02 filings just auto-passed with generic labels.

  Stage 3 (NEW): GPT-4o mini reads the full filing text and returns a JSON
           response with:
           - relevant: true/false (validates whether this is actually relevant)
           - category: "Management Change" / "Compensation" / "Both"
           - subcategory: "CEO Departure" / "New Hire" / etc.
           - summary: 2-3 sentence natural language summary

           If the LLM says relevant=false, the filing is dropped (even if
           keywords matched — this catches false positives).

           If the LLM call fails (API down, error, etc.), the system falls
           back to the old keyword classification and sentence scorer summary.
           The filing still gets stored, just with lower-quality data.

  Summary: Now generated by the LLM in Stage 3. The old sentence scorer in
           summarizer.py is kept as a fallback but is no longer the primary
           summary method.


FILES THAT WERE CHANGED
------------------------

  config.py (MODIFIED)
    Added these new settings:
    - OPENAI_API_KEY: Your OpenAI API key (line 98)
    - LLM_MODEL: Which model to use, currently "gpt-4o-mini" (line 101)
    - PROMPTS_DIR: Path to the prompts/ folder (line 105)
    - ACTIVE_PROMPT: Which prompt file the live pipeline uses (line 106)
    Everything else in config.py is unchanged.

  requirements.txt (MODIFIED)
    Added: openai>=1.0.0

  filter.py (MODIFIED)
    - Added imports for llm.py and summarizer.py at the top
    - The filter_filings() function now has 3 stages instead of 2
    - Stage 2 now tracks "near-misses" (5.02/8.01 filings that fail keywords)
    - Stage 3 sends keyword matches AND near-misses to the LLM
    - If LLM returns relevant=false, the filing is filtered out
    - If LLM returns relevant=true, its category/subcategory/summary are used
    - If LLM call fails, falls back to keyword category + sentence scorer
    - Functions stage1_item_code_filter() and stage2_keyword_filter() are UNCHANGED

  scheduler.py (MODIFIED)
    - The summary step now checks if a summary was already set by Stage 3
    - Only calls the old extract_summary() if no LLM summary exists
    - Changed lines 36-44

  repopulate.py (MODIFIED)
    - Same change as scheduler.py: checks for existing LLM summary before
      falling back to the old sentence scorer
    - Changed lines 38-46


FILES THAT WERE ADDED
---------------------

  llm.py (NEW)
    Contains two functions:
    - _load_prompt(prompt_file): Loads a prompt template from the prompts/ folder
    - classify_and_summarize(filing_text, prompt_file): Sends the filing text
      to OpenAI's API with the prompt, returns parsed JSON response.
      Uses temperature=0 for consistent results and JSON response format.
      Returns None if the API call fails.
      Also tracks token usage (_tokens_in, _tokens_out) in the response.

  prompts/prompt_v1.txt (NEW)
    The initial LLM prompt template. Contains instructions telling the model
    what to look for (executive departures, new hires, compensation events)
    and what JSON format to return. Has a {filing_text} placeholder that gets
    replaced with the actual filing content.

    To create a new prompt version, copy this file to prompt_v2.txt and edit it.
    To switch the live pipeline to a different prompt, change ACTIVE_PROMPT
    in config.py.

  test_prompt.py (NEW)
    A testing tool for iterating on prompts WITHOUT touching the database.
    Pulls filings from the existing database and runs them through the LLM,
    then prints a side-by-side comparison of old vs new results.

    Usage:
      python test_prompt.py                     Test active prompt on 10 filings
      python test_prompt.py --all               Test on all filings in database
      python test_prompt.py --count 20          Test on 20 filings
      python test_prompt.py --prompt prompt_v2.txt   Test a specific prompt
      python test_prompt.py --compare prompt_v1.txt prompt_v2.txt
                                                Compare two prompts side-by-side
      python test_prompt.py --list              List all available prompts

    The compare mode only shows filings where the two prompts disagree,
    making it easy to spot differences.

  prompts/ (NEW FOLDER)
    Stores prompt template files. Each file is a complete prompt with a
    {filing_text} placeholder. You can have as many versions as you want.


FILES THAT WERE NOT CHANGED
----------------------------
  summarizer.py   - Kept as-is. Used as fallback when LLM fails.
  database.py     - No schema changes. Existing columns work for LLM data.
  fetcher.py      - No changes. Still fetches from SEC EDGAR the same way.
  app.py          - No changes. Dashboard displays whatever is in the database.


HOW THE PROMPT SYSTEM WORKS
----------------------------
  1. Prompt files live in the prompts/ folder as plain text files
  2. ACTIVE_PROMPT in config.py controls which one the live pipeline uses
  3. test_prompt.py lets you test any prompt against real filings
  4. The --compare flag lets you run two prompts on the same filings and see
     where they disagree
  5. Workflow: edit prompt file --> run test_prompt.py --> check results -->
     repeat until happy --> set ACTIVE_PROMPT --> run repopulate.py


COST
----
  Model: GPT-4o mini ($0.15 per million input tokens, $0.60 per million output)
  Volume: ~7-8 filings per day hit the LLM
  Monthly cost: ~$0.23/month
  Annual cost: ~$2.75/year


FALLBACK BEHAVIOR
-----------------
  If the OpenAI API is unreachable or returns an error:
  - Filings that passed keyword filtering still get stored
  - They use the old keyword-based category/subcategory
  - They use the old sentence scorer summary
  - The pipeline does NOT crash — it prints a warning and continues
  - Near-miss filings (keyword failures) are simply skipped


DEPLOYMENT TO RENDER — CHANGELOG
=================================
Date: January 29, 2026

WHAT WE'RE DOING
-----------------
Deploying the app to Render.com (free tier) so it's accessible via a public URL.
Switching from SQLite (local file) to PostgreSQL (Render's free hosted database)
so data persists between deploys and server restarts.

WHY
---
To share the app with friends. SQLite doesn't survive Render's free tier server
recycling, so PostgreSQL is needed to keep filings, summaries, and user tags
permanently.


FILES MODIFIED
--------------

  requirements.txt (MODIFIED)
    BEFORE:
      flask==3.1.0
      requests==2.32.3
      beautifulsoup4==4.12.3
      schedule==1.2.2
      openai>=1.0.0

    AFTER: Added two lines:
      gunicorn==22.0.0        (production web server, replaces Flask's dev server)
      psycopg2-binary==2.9.9  (PostgreSQL driver for Python)

  config.py (MODIFIED)
    BEFORE:
      - OPENAI_API_KEY was hardcoded as a string on line 98
      - DATABASE_PATH = "filings.db" on line 109
    AFTER:
      - OPENAI_API_KEY reads from os.environ.get("OPENAI_API_KEY") with the old
        key as a fallback so local dev still works
      - Added DATABASE_URL that reads from os.environ.get("DATABASE_URL") with
        fallback to None (meaning "use SQLite locally")
      - DATABASE_PATH still exists for local SQLite fallback

  database.py (MODIFIED — biggest change)
    BEFORE: Used sqlite3 exclusively. All queries used ? placeholders.
    AFTER:
      - Uses psycopg2 for PostgreSQL when DATABASE_URL is set
      - Falls back to sqlite3 when DATABASE_URL is not set (local development)
      - Changed ? placeholders to %s for PostgreSQL compatibility
      - CREATE TABLE uses SERIAL PRIMARY KEY instead of INTEGER PRIMARY KEY AUTOINCREMENT
      - All function signatures (get_connection, initialize_database, filing_exists,
        insert_filing, get_filings, get_filing_by_id, update_user_tag, get_categories,
        get_filing_count) remain identical — same inputs, same outputs
      - Added dict-cursor support for psycopg2 (RealDictCursor)

  app.py (MODIFIED)
    BEFORE: app.run(debug=True, port=5000) on line 148
    AFTER:
      - debug=False (production mode)
      - host="0.0.0.0" (allows external connections)
      - port reads from os.environ.get("PORT", 5000) (Render sets PORT)


FILES ADDED
-----------
  .gitignore (NEW)
    Excludes: filings.db, .env, __pycache__/, *.pyc, venv/

  render.yaml (NEW)
    Render blueprint file. Defines the web service and PostgreSQL database
    in one config file for easy deployment.


HOW TO REVERT TO LOCAL-ONLY SQLITE VERSION
--------------------------------------------
If anything breaks and you want to go back to the original local-only app:

  1. requirements.txt: Remove the gunicorn and psycopg2-binary lines

  2. config.py: Replace the OPENAI_API_KEY line back to your hardcoded key:
       OPENAI_API_KEY = "your-key-here"
     (Your actual key is saved in your OpenAI account at platform.openai.com/api-keys)
     Remove the DATABASE_URL line entirely.

  3. database.py: Restore the original file. The original used sqlite3 only,
     with ? placeholders and INTEGER PRIMARY KEY AUTOINCREMENT. The full original
     contents are preserved above in the "FILES THAT WERE NOT CHANGED" section
     (database.py was listed as unchanged before this deployment work).

  4. app.py: Change line 148 back to:
       app.run(debug=True, port=5000)
     (remove host="0.0.0.0" and the os.environ PORT logic)

  5. Delete .gitignore and render.yaml (they're new files, not needed locally)

  6. Run: python app.py
     The app will work locally at http://127.0.0.1:5000 exactly as before.
